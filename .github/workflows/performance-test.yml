name: Performance Regression Testing

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:
    inputs:
      iterations:
        description: 'Number of Monte Carlo iterations'
        required: false
        default: '1000'
      pr_number:
        description: 'PR number to comment on (optional)'
        required: false

jobs:
  # Windows testing on self-hosted runner
  perf-test-windows:
    runs-on: self-hosted
    timeout-minutes: 60
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable
      
      - name: Generate test pages
        working-directory: test-infrastructure/pages
        run: python generate_test_pages.py
      
      - name: Build test harness
        working-directory: test-infrastructure/harness
        run: cargo build --release
      
      - name: Run performance tests
        working-directory: test-infrastructure/harness
        run: |
          $ITERATIONS = if ("${{ github.event.inputs.iterations }}") { "${{ github.event.inputs.iterations }}" } else { "1000" }
          ./target/release/hiwave-perf.exe --iterations $ITERATIONS --output perf-results-windows.json --baseline ../../../hiwave-windows/perf_baseline.json
        shell: pwsh
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: perf-results-windows
          path: test-infrastructure/harness/perf-results-windows.json
          retention-days: 30
      
      - name: Check for regressions
        working-directory: test-infrastructure/scripts
        run: |
          python regression-check.py ../harness/perf-results-windows.json --baseline ../../../hiwave-windows/perf_baseline.json
        continue-on-error: true

  # macOS testing on GitHub-hosted runner
  perf-test-macos:
    runs-on: macos-latest
    timeout-minutes: 60
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Python dependencies
        working-directory: test-infrastructure
        run: pip install -r requirements.txt
      
      - name: Generate test pages
        working-directory: test-infrastructure/pages
        run: python generate_test_pages.py
      
      - name: Build test harness
        working-directory: test-infrastructure/harness
        run: cargo build --release
      
      - name: Run performance tests
        working-directory: test-infrastructure/harness
        run: |
          ITERATIONS=${{ github.event.inputs.iterations }}
          ITERATIONS=${ITERATIONS:-1000}
          ./target/release/hiwave-perf --iterations $ITERATIONS --output perf-results-macos.json --baseline ../../../hiwave-macos/perf_baseline.json
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: perf-results-macos
          path: test-infrastructure/harness/perf-results-macos.json
          retention-days: 30
      
      - name: Check for regressions
        working-directory: test-infrastructure/scripts
        run: |
          python regression-check.py ../harness/perf-results-macos.json --baseline ../../../hiwave-macos/perf_baseline.json
        continue-on-error: true

  # Linux testing on GitHub-hosted runner
  perf-test-linux:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Python dependencies
        working-directory: test-infrastructure
        run: pip install -r requirements.txt
      
      - name: Generate test pages
        working-directory: test-infrastructure/pages
        run: python generate_test_pages.py
      
      - name: Build test harness
        working-directory: test-infrastructure/harness
        run: cargo build --release
      
      - name: Run performance tests
        working-directory: test-infrastructure/harness
        run: |
          ITERATIONS=${{ github.event.inputs.iterations }}
          ITERATIONS=${ITERATIONS:-1000}
          ./target/release/hiwave-perf --iterations $ITERATIONS --output perf-results-linux.json
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: perf-results-linux
          path: test-infrastructure/harness/perf-results-linux.json
          retention-days: 30

  # Aggregate and report results
  aggregate-results:
    needs: [perf-test-windows, perf-test-macos, perf-test-linux]
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
      issues: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Python dependencies
        working-directory: test-infrastructure
        run: pip install -r requirements.txt
      
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          path: test-results
      
      - name: Generate comprehensive report
        working-directory: test-infrastructure/scripts
        run: |
          python generate-report.py \
            ../../test-results/perf-results-windows/perf-results-windows.json \
            ../../test-results/perf-results-macos/perf-results-macos.json \
            ../../test-results/perf-results-linux/perf-results-linux.json \
            --output performance-report.md
      
      - name: Generate PR comment
        if: github.event.inputs.pr_number
        working-directory: test-infrastructure/scripts
        run: |
          python generate-report.py \
            ../../test-results/perf-results-windows/perf-results-windows.json \
            --pr-comment \
            --output pr-comment.md
      
      - name: Post PR comment
        if: github.event.inputs.pr_number
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('test-infrastructure/scripts/pr-comment.md', 'utf8');
            
            github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: ${{ github.event.inputs.pr_number }},
              body: comment
            });
      
      - name: Upload comprehensive report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: test-infrastructure/scripts/performance-report.md
          retention-days: 90
      
      - name: Check for critical regressions
        working-directory: test-results
        run: |
          # Exit with error if any platform had regressions
          if grep -q '"regressions":\s*\[' perf-results-*/perf-results-*.json; then
            echo "⚠️ Performance regressions detected!"
            exit 1
          fi
        shell: bash
        continue-on-error: true
