#!/usr/bin/env python3
"""
Generate comprehensive performance reports from test results.
Creates markdown reports suitable for GitHub PR comments or issues.
"""

import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List


def generate_markdown_report(results: Dict, platform_results: List[Dict] = None) -> str:
    """Generate a markdown performance report."""
    
    # Header
    md = ["# ğŸš€ HiWave Performance Test Report\n"]
    
    # Summary info
    md.append(f"**Platform:** {results['platform']}")
    md.append(f"**Iterations:** {results['iterations']:,}")
    md.append(f"**Duration:** {results['total_duration_secs']:.2f}s")
    md.append(f"**Git Commit:** `{results['git_commit']}`")
    md.append(f"**Timestamp:** {results['timestamp'][:19]}\n")
    
    # Regression status
    if results.get('regressions'):
        md.append(f"## âš ï¸ Regressions Detected: {len(results['regressions'])}\n")
        md.append("| Renderer | Metric | Change | Baseline | Current |")
        md.append("|----------|--------|--------|----------|---------|")
        for reg in results['regressions']:
            md.append(f"| {reg['renderer']} | {reg['metric']} | "
                     f"**{reg['percent_change']:+.2f}%** | "
                     f"{reg['baseline_value']:.2f} | {reg['current_value']:.2f} |")
        md.append("")
    else:
        md.append("## âœ… No Regressions Detected\n")
    
    # Renderer results
    md.append("## ğŸ“Š Performance Metrics\n")
    
    for renderer, stats in results['renderers'].items():
        md.append(f"### {renderer.upper()}\n")
        
        # Create table
        md.append("| Metric | Mean | Median | P95 | P99 | Min | Max |")
        md.append("|--------|------|--------|-----|-----|-----|-----|")
        
        metrics = [
            ('Parse Time', stats['parse_time'], 'ms'),
            ('Layout Time', stats['layout_time'], 'ms'),
            ('Paint Time', stats['paint_time'], 'ms'),
            ('**Total Time**', stats['total_time'], 'ms'),
            ('Memory', stats['memory'], 'MB'),
        ]
        
        for name, stat, unit in metrics:
            md.append(f"| {name} | {stat['mean']:.2f}{unit} | {stat['median']:.2f}{unit} | "
                     f"{stat['p95']:.2f}{unit} | {stat['p99']:.2f}{unit} | "
                     f"{stat['min']:.2f}{unit} | {stat['max']:.2f}{unit} |")
        
        md.append("")
    
    # Multi-platform comparison if available
    if platform_results and len(platform_results) > 1:
        md.append("## ğŸ–¥ï¸ Cross-Platform Comparison\n")
        md.append("### Total Render Time (Mean)\n")
        md.append("| Renderer | Windows | macOS | Linux |")
        md.append("|----------|---------|-------|-------|")
        
        # Extract renderer names
        renderers = set()
        for pr in platform_results:
            renderers.update(pr['renderers'].keys())
        
        for renderer in sorted(renderers):
            row = [renderer]
            for platform in ['windows', 'macos', 'linux']:
                pr = next((p for p in platform_results if p['platform'] == platform), None)
                if pr and renderer in pr['renderers']:
                    time = pr['renderers'][renderer]['total_time']['mean']
                    row.append(f"{time:.2f}ms")
                else:
                    row.append("N/A")
            md.append(f"| {' | '.join(row)} |")
        md.append("")
    
    # Footer
    md.append("\n---")
    md.append("*Generated by HiWave Performance Test Harness*")
    
    return "\n".join(md)


def generate_pr_comment(results: Dict, baseline_results: Dict = None) -> str:
    """Generate a concise PR comment."""
    
    lines = ["### ğŸš€ Performance Test Results\n"]
    
    # Quick summary
    lines.append(f"**Platform:** {results['platform']} | "
                f"**Iterations:** {results['iterations']:,} | "
                f"**Commit:** `{results['git_commit'][:7]}`\n")
    
    # Regression alert
    if results.get('regressions'):
        lines.append(f"#### âš ï¸ {len(results['regressions'])} Regression(s) Detected\n")
        for reg in results['regressions'][:3]:  # Show top 3
            lines.append(f"- **{reg['renderer']}** - {reg['metric']}: "
                        f"{reg['percent_change']:+.1f}% slower")
        if len(results['regressions']) > 3:
            lines.append(f"- *...and {len(results['regressions']) - 3} more*")
        lines.append("")
    else:
        lines.append("#### âœ… No Performance Regressions\n")
    
    # Quick stats table
    lines.append("<details>")
    lines.append("<summary>ğŸ“Š View Detailed Metrics</summary>\n")
    lines.append("| Renderer | Total Time | Memory |")
    lines.append("|----------|------------|--------|")
    
    for renderer, stats in results['renderers'].items():
        lines.append(f"| {renderer} | {stats['total_time']['mean']:.2f}ms | "
                    f"{stats['memory']['mean']:.2f}MB |")
    
    lines.append("\n</details>")
    
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description='Generate performance reports')
    parser.add_argument('results', type=Path, nargs='+', 
                       help='Test results JSON file(s)')
    parser.add_argument('--output', type=Path, default=Path('performance-report.md'),
                       help='Output markdown file')
    parser.add_argument('--pr-comment', action='store_true',
                       help='Generate concise PR comment format')
    parser.add_argument('--baseline', type=Path,
                       help='Baseline results for comparison')
    args = parser.parse_args()
    
    # Load results
    results_list = []
    for results_file in args.results:
        with open(results_file) as f:
            results_list.append(json.load(f))
    
    # Load baseline if provided
    baseline = None
    if args.baseline and args.baseline.exists():
        with open(args.baseline) as f:
            baseline = json.load(f)
    
    # Generate report
    if args.pr_comment:
        # Use first result for PR comment
        report = generate_pr_comment(results_list[0], baseline)
    else:
        # Use first result, but pass all for cross-platform comparison
        report = generate_markdown_report(
            results_list[0],
            platform_results=results_list if len(results_list) > 1 else None
        )
    
    # Write output
    with open(args.output, 'w') as f:
        f.write(report)
    
    print(f"ğŸ“„ Report generated: {args.output}")
    
    # Also print to console
    print("\n" + report)


if __name__ == '__main__':
    main()
